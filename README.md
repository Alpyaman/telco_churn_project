# Customer Churn Prediction - Telco Dataset ğŸ“‰ğŸ”

## ğŸ“Œ Project Overview

This project aims to predict **customer churn** using a real-world Telco dataset. The objective is to help telecom companies **identify customers at risk of leaving**, allowing them to proactively take retention measures. This end-to-end machine learning pipeline demonstrates my ability to handle imbalanced classification problems with modern techniques like **SMOTE, hyperparameter tuning (GridSearchCV)**, and **ensemble modeling (VotingClassifier)**.

---

## ğŸ“Š Dataset Summary

- **Source**: IBM Sample Data â€“ [Telco Customer Churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
- **Rows**: 7,043
- **Target**: `Churn` (Yes/No â†’ encoded as 1/0)
- **Features Used**:
  - Numerical: `tenure`, `MonthlyCharges`, `TotalCharges`
  - Categorical: `Contract`, `PaymentMethod`, `InternetService`, `OnlineSecurity`, `TechSupport`, `gender`, `PaperlessBilling`, `OnlineBackup`

---

## ğŸ› ï¸ Installation

Clone the repo and install required dependencies:

```bash
git clone https://github.com/yourusername/telco-churn-prediction.git
cd telco-churn-prediction
pip install -r requirements.txt
```
## ğŸ“ Project Structure
```bash
telco-churn-prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚
â”œâ”€â”€ preprocess.py          # Preprocessing pipeline
â”œâ”€â”€ train_model.py         # Model training and evaluation script
â”œâ”€â”€ requirements.txt       # Required Python libraries
â”œâ”€â”€ README.md              # Project documentation
```
## ğŸš€ Usage
To train the model and evaulate performance:
```bash
python train_model.py
```
This script:
- Preprocess data
- Handles class imbalance using `SMOTE`
- Tunes models using `GridSearchCV`
- Evaulates final predictions using a `VotingClassifier`
- Plots `ROC` and `Precision-Recall` curves

## ğŸ§  Models & Techniques
| Model            | Optimized With | Metric   |
| ---------------- | -------------- | -------- |
| RandomForest     | GridSearchCV   | F1-Score |
| LightGBM         | GridSearchCV   | F1-Score |
| XGBoost          | GridSearchCV   | F1-Score |
| GradientBoosting | GridSearchCV   | F1-Score |
| VotingClassifier | Soft Voting    | ROC-AUC  |

### ğŸ“Œ Best Performance:
- *VotingClassifier* (Ensemble of best RF, LGBM, XGBoost, GradientBooster)
- *ROC AUC Score*: `0.81`
- *F1 Score(Churn Class)*: `0.58`
- *Classification Accuracy: `76%`

## ğŸ¯ Skills Highlighted
âœ… Data Cleaning & EDA
âœ… Feature Engineering & Selection
âœ… Imbalanced Learning with SMOTE
âœ… Hyperparameter Tuning (GridSearchCV)
âœ… Advanced ML Models: LightGBM, XGBoost, Ensembles
âœ… ROC/PR Curve Interpretation
âœ… Production-ready code structure

## ğŸ”® Future Improvements
- Model explainability with SHAP/LIME
- Flask or Streamlit API deployment
- Integration with live churn dashboard

## ğŸ‘¤ About Me
- Iâ€™m a data-driven problem solver with a passion for transforming raw data into actionable insights. This project reflects my proficiency in classification tasks, real-world deployment pipelines, and business impact-focused modeling.
- ğŸ”— [Linkedln](https://www.linkedin.com/in/alp-yaman-75a901174/) | ğŸ“§ alpyaman3@gmail.com




